{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polished-drilling",
   "metadata": {
    "id": "polished-drilling"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "returning-judgment",
   "metadata": {
    "id": "returning-judgment"
   },
   "outputs": [],
   "source": [
    "DS_PATH = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genetic-acoustic",
   "metadata": {
    "id": "genetic-acoustic"
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(DS_PATH, 'Train')\n",
    "test_dir = os.path.join(DS_PATH, 'TestR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "trained-hindu",
   "metadata": {
    "id": "trained-hindu"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Qzk9biMi13Fu",
   "metadata": {
    "id": "Qzk9biMi13Fu"
   },
   "outputs": [],
   "source": [
    "class_no = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-purpose",
   "metadata": {
    "id": "sublime-purpose"
   },
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "primary-memorabilia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "primary-memorabilia",
    "outputId": "c2465ec8-34a6-4c27-9e7d-fd9af008073e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 files belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                            shuffle=True,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interstate-district",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "interstate-district",
    "outputId": "fa71b405-a1e9-4627-a7d6-62a6cf684706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12630 files belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           image_size=IMG_SIZE,\n",
    "                                                           label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wicked-packet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "wicked-packet",
    "outputId": "34a5ca4f-32d1-43be-d091-c0d127d48b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "print(class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[np.argmax(labels[i].numpy())])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "plt.savefig(\"./plots/train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facial-access",
   "metadata": {
    "id": "facial-access"
   },
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "theoretical-atlanta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "theoretical-atlanta",
    "outputId": "840fe980-58b2-4a01-aa0d-dc1957cd1f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 316\n",
      "Number of test batches: 79\n"
     ]
    }
   ],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "herbal-grant",
   "metadata": {
    "id": "herbal-grant"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-climate",
   "metadata": {
    "id": "micro-climate"
   },
   "source": [
    "## Preprocessing for Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "corporate-sellers",
   "metadata": {
    "id": "corporate-sellers"
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.resnet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-hierarchy",
   "metadata": {
    "id": "forced-hierarchy"
   },
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thorough-grove",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thorough-grove",
    "outputId": "0fb43d96-1249-4106-f41a-e7f8c28928dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94674944/94668760 [==============================] - 1s 0us/step\n",
      "94683136/94668760 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Create the base model from the pre-trained model ResNet V2\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.ResNet50V2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "natural-spyware",
   "metadata": {
    "id": "natural-spyware"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "knowing-locking",
   "metadata": {
    "id": "knowing-locking"
   },
   "outputs": [],
   "source": [
    "global_max_layer = tf.keras.layers.GlobalMaxPooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smooth-worst",
   "metadata": {
    "id": "smooth-worst"
   },
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(class_no, activation='softmax', use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "divided-campaign",
   "metadata": {
    "id": "divided-campaign"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = global_max_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "challenging-carnival",
   "metadata": {
    "id": "challenging-carnival"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics = tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solid-concrete",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "solid-concrete",
    "outputId": "13010419-f996-4aa1-c8e0-0b1165416a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda) (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "resnet50v2 (Functional)      (None, 1, 1, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 43)                88107     \n",
      "=================================================================\n",
      "Total params: 23,652,907\n",
      "Trainable params: 88,107\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eastern-verse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eastern-verse",
    "outputId": "867d08c1-13dc-4b05-d3eb-e0d0c6642322"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affecting-charleston",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affecting-charleston",
    "outputId": "8419df12-dcad-4d2f-9976-e8e79fc839eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 32s 76ms/step - loss: 3.8350 - categorical_accuracy: 0.0212\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 20\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "atomic-electron",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atomic-electron",
    "outputId": "381221e2-f398-4b4c-9f8c-e61d8bf80a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 3.84\n",
      "initial accuracy: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "boolean-resistance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boolean-resistance",
    "outputId": "2345ee96-f3ee-42c2-b8c5-1636735019b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1226/1226 [==============================] - 242s 195ms/step - loss: 3.0646 - categorical_accuracy: 0.1668 - val_loss: 2.7949 - val_categorical_accuracy: 0.2477\n",
      "Epoch 2/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 2.5558 - categorical_accuracy: 0.2943 - val_loss: 2.5720 - val_categorical_accuracy: 0.3072\n",
      "Epoch 3/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 2.3422 - categorical_accuracy: 0.3467 - val_loss: 2.4477 - val_categorical_accuracy: 0.3358\n",
      "Epoch 4/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 2.2036 - categorical_accuracy: 0.3810 - val_loss: 2.3734 - val_categorical_accuracy: 0.3469\n",
      "Epoch 5/20\n",
      "1226/1226 [==============================] - 67s 54ms/step - loss: 2.1023 - categorical_accuracy: 0.4040 - val_loss: 2.3264 - val_categorical_accuracy: 0.3558\n",
      "Epoch 6/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 2.0291 - categorical_accuracy: 0.4223 - val_loss: 2.2890 - val_categorical_accuracy: 0.3623\n",
      "Epoch 7/20\n",
      "1226/1226 [==============================] - 68s 56ms/step - loss: 1.9668 - categorical_accuracy: 0.4345 - val_loss: 2.2604 - val_categorical_accuracy: 0.3648\n",
      "Epoch 8/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 1.9118 - categorical_accuracy: 0.4491 - val_loss: 2.2399 - val_categorical_accuracy: 0.3671\n",
      "Epoch 9/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 1.8704 - categorical_accuracy: 0.4573 - val_loss: 2.2260 - val_categorical_accuracy: 0.3691\n",
      "Epoch 10/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 1.8348 - categorical_accuracy: 0.4656 - val_loss: 2.2186 - val_categorical_accuracy: 0.3702\n",
      "Epoch 11/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.8048 - categorical_accuracy: 0.4696 - val_loss: 2.2062 - val_categorical_accuracy: 0.3731\n",
      "Epoch 12/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.7734 - categorical_accuracy: 0.4792 - val_loss: 2.2002 - val_categorical_accuracy: 0.3769\n",
      "Epoch 13/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.7506 - categorical_accuracy: 0.4815 - val_loss: 2.1928 - val_categorical_accuracy: 0.3797\n",
      "Epoch 14/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 1.7261 - categorical_accuracy: 0.4906 - val_loss: 2.1864 - val_categorical_accuracy: 0.3826\n",
      "Epoch 15/20\n",
      "1226/1226 [==============================] - 67s 55ms/step - loss: 1.7036 - categorical_accuracy: 0.4962 - val_loss: 2.1881 - val_categorical_accuracy: 0.3820\n",
      "Epoch 16/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.6850 - categorical_accuracy: 0.4963 - val_loss: 2.1828 - val_categorical_accuracy: 0.3830\n",
      "Epoch 17/20\n",
      "1226/1226 [==============================] - 67s 54ms/step - loss: 1.6672 - categorical_accuracy: 0.5028 - val_loss: 2.1772 - val_categorical_accuracy: 0.3856\n",
      "Epoch 18/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.6517 - categorical_accuracy: 0.5046 - val_loss: 2.1802 - val_categorical_accuracy: 0.3872\n",
      "Epoch 19/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.6392 - categorical_accuracy: 0.5087 - val_loss: 2.1801 - val_categorical_accuracy: 0.3881\n",
      "Epoch 20/20\n",
      "1226/1226 [==============================] - 68s 55ms/step - loss: 1.6268 - categorical_accuracy: 0.5112 - val_loss: 2.1754 - val_categorical_accuracy: 0.3896\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "WKwQZpOQLxl6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKwQZpOQLxl6",
    "outputId": "0d8e3449-5dec-48a6-ab4d-dcb019e22243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/base_resnet50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/software/local/python/3.9/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/base_resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "protective-copyright",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "protective-copyright",
    "outputId": "a7c3ac33-7a0a-4fca-c647-41407ab01ef6"
   },
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,4])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('./plots/TrainingHistory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "metallic-cause",
   "metadata": {
    "id": "metallic-cause"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "classified-treasury",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "classified-treasury",
    "outputId": "a401a838-86e6-440c-a9ce-0a155f0a4cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  190\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "welsh-colonial",
   "metadata": {
    "id": "welsh-colonial"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics = tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "operational-timber",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "operational-timber",
    "outputId": "08df9e6d-9115-4ce9-c517-bef553b5bb5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "raised-dancing",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "raised-dancing",
    "outputId": "af512b82-339f-4464-f633-60847be18efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40\n",
      "1226/1226 [==============================] - 1379s 1s/step - loss: 4.5116 - categorical_accuracy: 0.1131 - val_loss: 4.3756 - val_categorical_accuracy: 0.1518\n",
      "Epoch 21/40\n",
      "1226/1226 [==============================] - 1375s 1s/step - loss: 3.0212 - categorical_accuracy: 0.2019 - val_loss: 3.2481 - val_categorical_accuracy: 0.2569\n",
      "Epoch 22/40\n",
      "1226/1226 [==============================] - 1367s 1s/step - loss: 2.5021 - categorical_accuracy: 0.3020 - val_loss: 3.1720 - val_categorical_accuracy: 0.3297\n",
      "Epoch 23/40\n",
      "1226/1226 [==============================] - 1363s 1s/step - loss: 2.0858 - categorical_accuracy: 0.3949 - val_loss: 2.5237 - val_categorical_accuracy: 0.3927\n",
      "Epoch 24/40\n",
      "1226/1226 [==============================] - 1363s 1s/step - loss: 1.7746 - categorical_accuracy: 0.4652 - val_loss: 2.5064 - val_categorical_accuracy: 0.4369\n",
      "Epoch 25/40\n",
      "1226/1226 [==============================] - 1360s 1s/step - loss: 1.5321 - categorical_accuracy: 0.5324 - val_loss: 2.5932 - val_categorical_accuracy: 0.4888\n",
      "Epoch 26/40\n",
      "1226/1226 [==============================] - 1364s 1s/step - loss: 1.3325 - categorical_accuracy: 0.5868 - val_loss: 2.6056 - val_categorical_accuracy: 0.5315\n",
      "Epoch 27/40\n",
      "1226/1226 [==============================] - 1367s 1s/step - loss: 1.1449 - categorical_accuracy: 0.6416 - val_loss: 2.0101 - val_categorical_accuracy: 0.5605\n",
      "Epoch 28/40\n",
      "1226/1226 [==============================] - 1367s 1s/step - loss: 0.9842 - categorical_accuracy: 0.6937 - val_loss: 1.5910 - val_categorical_accuracy: 0.5911\n",
      "Epoch 29/40\n",
      "1226/1226 [==============================] - 1363s 1s/step - loss: 0.8398 - categorical_accuracy: 0.7384 - val_loss: 1.3753 - val_categorical_accuracy: 0.6219\n",
      "Epoch 30/40\n",
      "1226/1226 [==============================] - 1365s 1s/step - loss: 0.7086 - categorical_accuracy: 0.7781 - val_loss: 1.3153 - val_categorical_accuracy: 0.6380\n",
      "Epoch 31/40\n",
      "1226/1226 [==============================] - 1364s 1s/step - loss: 0.6110 - categorical_accuracy: 0.8084 - val_loss: 1.7041 - val_categorical_accuracy: 0.6394\n",
      "Epoch 32/40\n",
      "1226/1226 [==============================] - 1364s 1s/step - loss: 0.5159 - categorical_accuracy: 0.8365 - val_loss: 1.8540 - val_categorical_accuracy: 0.6650\n",
      "Epoch 33/40\n",
      "1226/1226 [==============================] - 1366s 1s/step - loss: 0.4396 - categorical_accuracy: 0.8619 - val_loss: 2.0564 - val_categorical_accuracy: 0.6722\n",
      "Epoch 34/40\n",
      "1226/1226 [==============================] - 1367s 1s/step - loss: 0.3761 - categorical_accuracy: 0.8812 - val_loss: 1.8654 - val_categorical_accuracy: 0.6792\n",
      "Epoch 35/40\n",
      "1226/1226 [==============================] - 1364s 1s/step - loss: 0.3242 - categorical_accuracy: 0.8987 - val_loss: 1.3490 - val_categorical_accuracy: 0.6925\n",
      "Epoch 36/40\n",
      "1226/1226 [==============================] - 1363s 1s/step - loss: 0.2776 - categorical_accuracy: 0.9127 - val_loss: 1.6524 - val_categorical_accuracy: 0.7035\n",
      "Epoch 37/40\n",
      "1226/1226 [==============================] - 1363s 1s/step - loss: 0.2344 - categorical_accuracy: 0.9259 - val_loss: 1.8832 - val_categorical_accuracy: 0.7050\n",
      "Epoch 38/40\n",
      "1226/1226 [==============================] - 1364s 1s/step - loss: 0.2089 - categorical_accuracy: 0.9348 - val_loss: 3.5540 - val_categorical_accuracy: 0.6979\n",
      "Epoch 39/40\n",
      "1226/1226 [==============================] - 1362s 1s/step - loss: 0.1843 - categorical_accuracy: 0.9419 - val_loss: 1.9307 - val_categorical_accuracy: 0.7084\n",
      "Epoch 40/40\n",
      "1226/1226 [==============================] - 1362s 1s/step - loss: 0.1576 - categorical_accuracy: 0.9500 - val_loss: 3.7950 - val_categorical_accuracy: 0.6971\n"
     ]
    }
   ],
   "source": [
    "fine_tune_epochs = 20\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "UzkR7kyoEF5u",
   "metadata": {
    "id": "UzkR7kyoEF5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/base_resnet50_fitted/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/software/local/python/3.9/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/base_resnet50_fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "silver-wiring",
   "metadata": {
    "id": "silver-wiring"
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['categorical_accuracy']\n",
    "val_acc += history_fine.history['val_categorical_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "horizontal-armor",
   "metadata": {
    "id": "horizontal-armor"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "plt.savefig('./plots/fullHistory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "focal-guinea",
   "metadata": {
    "id": "focal-guinea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 3s 43ms/step - loss: 3.4242 - categorical_accuracy: 0.7033\n",
      "Test accuracy : 0.7033227682113647\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "initial-greenhouse",
   "metadata": {
    "id": "initial-greenhouse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [1 1 1 ... 1 1 1]\n",
      "Labels:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "# Apply a sigmoid since our model returns logits\n",
    "predictions = tf.nn.sigmoid(predictions)\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")\n",
    "plt.savefig('./plots/scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-cycle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
